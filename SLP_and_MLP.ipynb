{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of ML_Lab3_Final.ipynb","provenance":[{"file_id":"1dgffFkw4GXD2ThZyVXLv3l6ZiqQvGRAO","timestamp":1637104785135},{"file_id":"1elHsN3VYMD1gw1gWa2ZsZCaIc1Mrcvkw","timestamp":1637103485992}],"collapsed_sections":[],"authorship_tag":"ABX9TyOoLtirp3F3zdWzWavhtv18"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-ta-MirroIov"},"source":["\n","#### Q1. (i) In this simple assignment, you will implement a single layered perceptron algorithm. You will need to write in a language of your choice from SCRATCH (use of pre-built machine learning libraries is not allowed though you can use basic linear algebra routines from numpy and scipy). \n","#### (ii) Then use the MNIST dataset to find the accuracy that a single layered perceptron that you have implemented gives. Note the MNIST dataset requires classification into one of 10 categories (digits). You may assume that the class indicated is given by the output of the corresponding neuron with the highest output. Use 5-fold cross validation"]},{"cell_type":"markdown","metadata":{"id":"voTdAkYS-fYl"},"source":["## Creating the Single Layer Perceptron"]},{"cell_type":"code","metadata":{"id":"7WXqgyxmnud2"},"source":["# Importing Libraries\n","\n","import numpy as np\n","import scipy as sp\n","import math\n","import random\n","import time\n","\n","# Creating class\n","\n","class SLP():\n","\n","    def __init__(self, nodes, types = 'online', alpha=0.001, max_iter=500, tol=0.0001, activation='relu'):\n","        if activation.lower() not in ['relu','binsig', 'bipsig','linear']:\n","            raise Exception(\"Activation should be either Relu, Linear or Binary/Bipolar Sigmoid\")\n","        if types.lower() not in ['online','batch']:\n","            raise Exception(\"Learning should be either Online or Batch\")\n","        if alpha<=0 or max_iter<=0 or tol<0 or nodes<1:\n","            raise Exception(\"Parameter values cannot be negative.\")\n","        self.alpha = alpha\n","        self.max_iter = max_iter\n","        self.tol = tol\n","        self.activation = activation\n","        self.nodes = nodes\n","        self.X_train = []\n","        self.y_train = []\n","        self.w_arr = []\n","        self.iters = 0\n","        self.J = 100000\n","    \n","    def activation_function(self, weighted_sum):\n","        if self.activation.lower() == 'linear':\n","            return weighted_sum\n","        elif self.activation.lower() == 'relu':\n","            return max(0,weighted_sum)\n","        elif self.activation.lower() == 'binsig':\n","            if weighted_sum>100:\n","                return 0\n","            elif weighted_sum<-100:\n","                return 1\n","            else:\n","                return 1/(1+math.exp(weighted_sum*-1))\n","        elif self.activation.lower() == 'bipsig':\n","            if weighted_sum>100:\n","                return 0\n","            elif weighted_sum<-100:\n","                return 1\n","            else:\n","                return (2/(1+math.exp(weighted_sum*-1))) - 1\n","\n","    def delJ(self, y_pred):\n","        if self.activation.lower() == 'linear':\n","            delJ = 1\n","        elif self.activation.lower() == 'relu':\n","            delJ = 1\n","            # How does one take this out for Relu???\n","        elif self.activation.lower() == 'binsig':\n","            delJ = 0\n","            for node in range(self.nodes):\n","                delJ = delJ + y_pred[node]*(1-y_pred[node])\n","        elif self.activation.lower() == 'bipsig':\n","            delJ = 0\n","            for node in range(self.nodes):\n","                delJ = delJ + ((1-y_pred[node])**2)/2\n","        return delJ\n","\n","    def predict(self, X_test):\n","        # Inserting a column of 1s at the 0th index\n","        result = []\n","        bias = np.ones(X_test.shape[0])\n","        X_test = np.insert(X_test, 0, bias, axis=1)\n","        for j in range(X_test.shape[0]): # 70000\n","                y_actual = y_train[j]\n","                y_pred = []\n","                for node in range(self.nodes): # 10\n","                    weighted_sum = X_test[j] @ self.w_arr[:,node]\n","                    y_val = self.activation_function(weighted_sum)\n","                    y_pred.append(y_val)\n","                result.append(y_pred)\n","        return result\n","\n","    def score(self, y_test, y_pred):\n","        count = 0\n","        total = len(y_test)\n","        for i in range(total):\n","            if y_test == y_pred:\n","                count += 1\n","        return count/total\n","\n","    def forward_pass(self):\n","        # the while loop code\n","        pass\n","\n","    def weight_adjust(self):\n","        # second half of while loop\n","        pass\n","\n","    def fit(self, X_train, y_train):\n","        t1 = time.time()\n","        X_train = np.array(X_train)\n","        y_train = np.array(y_train)\n","        input_neurons = X_train.shape[1]              #785\n","        output_neurons = np.zeros(y_train.max()+1)    #10\n","\n","        b = np.zeros((y_train.size, y_train.max()+1))\n","        b[np.arange(y_train.size), y_train] = 1\n","        y_train = b\n","\n","        # Inserting a column of 1s at the 0th index\n","        bias = np.ones(X_train.shape[0])\n","        X_train = np.insert(X_train, 0, bias, axis=1)\n","\n","        if X_train.ndim > 2:\n","            raise Exception(\"X cannot be a\",X_train.ndim,\"dimensional array.\")\n","        \n","        # Creating initial weights. \n","        for node in range(self.nodes):\n","            W = []\n","            # Creating a 785 x 1 sized weight vector (x0 = 0) for EACH NODE (785x5 if 5 nodes)\n","            for i in range(X_train.shape[1]):\n","                W.append(random.randint(-250,250)/1000)\n","            self.w_arr.append(W)\n","        self.w_arr = np.transpose(self.w_arr)\n","\n","        # Machine is learning\n","        while self.iters < self.max_iter and self.J > self.tol:\n","            J_val = 0\n","\n","            for j in range(X_train.shape[0]): # 70000\n","                y_actual = y_train[j]\n","                y_pred = []\n","\n","                for node in range(self.nodes): # `0\n","                    weighted_sum = X_train[j] @ self.w_arr[:,node]\n","                    y_val = self.activation_function(weighted_sum)\n","                    y_pred.append(y_val)\n","                    J_val = J_val + (y_actual[node] - y_pred[node])**2\n","\n","                # assume that the class indicated is given by the output of the corresponding neuron with the highest output.\n","                y_pred = np.array(y_pred)\n","                b = np.zeros(self.nodes)\n","                b[y_pred.argmax()] = 1\n","                y_pred = b\n","\n","                for node in range(self.nodes):\n","                    for z in range(X_train.shape[1]): # 785\n","                        delta = self.alpha * self.delJ(y_pred) * (y_actual[node] - y_pred[node]) * X_train[j,z]\n","                        self.w_arr[z,node] = self.w_arr[z,node] - delta\n","            self.iters += 1\n","            self.J = J_val\n","        t2 = time.time()\n","        print(\"Time taken by algorithm:\", t2 - t1)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9qLkf8r-wig"},"source":["## Creating the Cross Validation class"]},{"cell_type":"code","metadata":{"id":"rNHoinyHJqUv"},"source":["class cross_validate():\n","\n","    def __init__(self, splits):\n","        self.splits = splits\n","\n","    def split(self, X):\n","        indices = []\n","        size = int(X.shape[0]/self.splits)\n","        test1 = 0\n","        test2 = size\n","        for i in range(self.splits):\n","            mask = np.full(X.shape[0], True, dtype = bool)\n","            index = np.arange(test1,test2)\n","            mask[index] = False\n","            test1 += size\n","            test2 += size\n","            indices.append([mask,~mask])\n","        return indices\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bg_EqHhE-7V4"},"source":["## Testing our algorithm"]},{"cell_type":"code","metadata":{"id":"ymQoDYwWoS7Q"},"source":["from keras.datasets import mnist\n","\n","(trainX, trainy), (testX, testy) = mnist.load_data()\n","X = np.concatenate([trainX, testX], axis=0)\n","y = np.concatenate([trainy, testy], axis=0)\n","X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n","\n","slp = SLP(10, types = 'online', alpha=0.001, max_iter=300, tol=0.001, activation='binsig')\n","cv = cross_validate(5)\n","scores = []\n","\n","for train, test in cv.split(X):\n","    X_train, X_test = X[train], X[test]\n","    y_train, y_test = y[train], y[test]\n","    slp.fit(X_train, y_train)\n","    # y_pred = slp.predict(X_test)\n","    score = slp.score(X_test, y_test)\n","    scores.append(score)\n","\n","scores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtK5eMDkoAwU"},"source":["#### Q2. For this part you can use scikit (or any other pre-packaged library) and use their built-in implementation of multi-layered perceptron. Use the same dataset as above. Use 5-fold cross validation as above and compare your results with what you get with a single layered perceptron\n"]},{"cell_type":"code","metadata":{"id":"Mzo3N8uTn75q","executionInfo":{"status":"ok","timestamp":1637104847546,"user_tz":-330,"elapsed":3284,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12980308010814139128"}}},"source":["import numpy as np\n","from tensorflow.keras.utils import to_categorical\n","from keras.datasets import mnist\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.model_selection import KFold\n","from sklearn import metrics\n","from sklearn.preprocessing import OneHotEncoder"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wvpEDswcn7-D","executionInfo":{"status":"ok","timestamp":1637104848192,"user_tz":-330,"elapsed":658,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12980308010814139128"}},"outputId":"f39b9017-2267-48c0-da05-35a514d1ac0a"},"source":["# Loading the Dataset and preprocessing\n","(trainX, trainy), (testX, testy) = mnist.load_data()\n","\n","# Concatenating the two datasets given we plan on using k-Fold Cross Validation\n","X = np.concatenate([trainX, testX], axis=0)\n","y = np.concatenate([trainy, testy], axis=0)\n","print(\"X:\",len(X),\"y:\",len(y))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","X: 70000 y: 70000\n"]}]},{"cell_type":"code","metadata":{"id":"0koA8h9gp9X6","executionInfo":{"status":"ok","timestamp":1637104867279,"user_tz":-330,"elapsed":418,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12980308010814139128"}}},"source":["# Creating model\n","# Here we use logistic activation - Binary Sigmoid given how it returns a value between 0 and 1, which we want.\n","\n","clf = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(10), max_iter=5000, tol=0.00001, activation='logistic')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yteAOatLqgWT","executionInfo":{"status":"ok","timestamp":1637104868760,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12980308010814139128"}},"outputId":"aad6662d-e1a3-4521-9fae-5e446d627b77"},"source":["# Preprocessing - Converting X to a 2D array\n","# X currently is a 3D array with first dimension being the no of examples,\n","# while the second and third dimensions are the two dimensions of pixels, making a 28x28 matrix\n","# So we flatten this array to a vector of size 784\n","\n","# Thanks to this source which helped me understand why my code wasn't initially working: \n","# https://medium.com/analytics-vidhya/multi-layer-perceptron-using-keras-on-mnist-dataset-for-digit-classification-problem-relu-a276cbf05e97\n","\n","X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])\n","X.shape"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70000, 784)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"8sUFybjcn8Bt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637105121097,"user_tz":-330,"elapsed":250217,"user":{"displayName":"Sarthak Arora","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12980308010814139128"}},"outputId":"b35bb612-0673-41c4-be56-2c92c9ba5082"},"source":["# Running 5 fold classification and checking score for each classifier\n","\n","kf = KFold(n_splits=5)\n","scores = []\n","for train, test in kf.split(X):\n","    X_train, X_test = X[train], X[test]\n","    y_train, y_test = y[train], y[test]\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","    # matrix = metrics.confusion_matrix(y_test, y_pred)\n","    score = clf.score(X_test, y_test)\n","    scores.append(score)\n","\n","scores"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.8924285714285715,\n"," 0.8806428571428572,\n"," 0.887,\n"," 0.8877142857142857,\n"," 0.9139285714285714]"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Yql88LogdgbW"},"source":[""],"execution_count":null,"outputs":[]}]}